{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3bd1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Dict, Any, List, Optional\n",
    "from IPython.display import display_markdown\n",
    "from pathlib import Path\n",
    "import os, re, json, math, time\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# ---------- setup ----------\n",
    "load_dotenv()\n",
    "TAVILY = TavilyClient(os.getenv(\"TAVILY_API_KEY\"))\n",
    "_USE_TAVILY_ANSWER_FALLBACK = False\n",
    "_MIN_SUMMARY_CHARS = 300\n",
    "\n",
    "# ---------- text utils ----------\n",
    "_STOP = {\"the\",\"a\",\"an\",\"and\",\"or\",\"if\",\"with\",\"to\",\"of\",\"in\",\"on\",\"for\",\"by\",\"from\",\"as\",\n",
    "         \"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"it\",\"its\",\"this\",\"that\",\"at\",\"about\",\n",
    "         \"we\",\"you\",\"they\",\"he\",\"she\",\"their\",\"our\",\"us\"}\n",
    "_SENT_SPLIT = re.compile(r'(?<=[\\.\\?\\!])\\s+')\n",
    "\n",
    "def _normalize(t:str)->str: return re.sub(r\"\\s+\",\" \",t or \"\").strip()\n",
    "def _tok(t:str)->List[str]: return re.findall(r\"[A-Za-z0-9']+\",t.lower())\n",
    "\n",
    "def _score_sents(sents:List[str])->Dict[int,float]:\n",
    "    freq={}\n",
    "    for s in sents:\n",
    "        for w in _tok(s):\n",
    "            if w in _STOP: continue\n",
    "            freq[w]=freq.get(w,0)+1\n",
    "    if not freq: return {i:0 for i in range(len(sents))}\n",
    "    mx=max(freq.values())\n",
    "    tf={w:(0.5+0.5*freq[w]/mx) for w in freq}\n",
    "    out={}\n",
    "    for i,s in enumerate(sents):\n",
    "        sc=sum(tf.get(w,0) for w in _tok(s))\n",
    "        L=len(s)\n",
    "        out[i]=sc*(1.0 if 60<=L<=240 else 0.8)\n",
    "    return out\n",
    "\n",
    "def _mmr(sents: List[str], scores: Dict[int, float], k: int = 6, div: float = 0.7) -> List[int]:\n",
    "    selected: List[int] = []\n",
    "    candidates = set(range(len(sents)))\n",
    "\n",
    "    def sim(a: str, b: str) -> float:\n",
    "        A, B = set(_tok(a)), set(_tok(b))\n",
    "        if not A or not B:\n",
    "            return 0.0\n",
    "        return len(A & B) / math.sqrt(len(A) * len(B))\n",
    "\n",
    "    while candidates and len(selected) < k:\n",
    "        best_i = None\n",
    "        best_val = float(\"-inf\")\n",
    "        for i in candidates:\n",
    "            relevance = scores.get(i, 0.0)\n",
    "            redundancy = max((sim(sents[i], sents[j]) for j in selected), default=0.0)\n",
    "            val = div * relevance - (1.0 - div) * redundancy\n",
    "            if val > best_val:\n",
    "                best_val = val\n",
    "                best_i = i\n",
    "\n",
    "        if best_i is None:          # safety: shouldn't happen, but avoid KeyError\n",
    "            break\n",
    "        selected.append(best_i)\n",
    "        candidates.remove(best_i)\n",
    "\n",
    "    return sorted(selected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e37af39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict, total=False):\n",
    "    question: str\n",
    "    raw_results: Dict[str,Any]\n",
    "    articles: List[Dict[str,Any]]\n",
    "    answer: str\n",
    "    error: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "727f84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_web(state:AgentState)->AgentState:\n",
    "    q=(state.get(\"question\") or \"\").strip()\n",
    "    if not q: return {\"error\":\"Empty question.\"}\n",
    "    try:\n",
    "        res=TAVILY.search(\n",
    "            query=q,max_results=8,\n",
    "            include_answer=False,\n",
    "            include_raw_content=True,\n",
    "            search_depth=\"advanced\"\n",
    "        ) or {}\n",
    "    except Exception as e:\n",
    "        return {\"error\":f\"Tavily search failed: {e}\"}\n",
    "\n",
    "    results=res.get(\"results\") or []\n",
    "    urls=[r[\"url\"] for r in results if r.get(\"url\") and not r.get(\"content\")]\n",
    "    extra={}\n",
    "    if urls:\n",
    "        try:\n",
    "            ext=TAVILY.extract(urls=urls) or []\n",
    "            extra={e.get(\"url\"):(e.get(\"content\") or \"\") for e in ext}\n",
    "        except Exception: pass\n",
    "\n",
    "    arts=[]\n",
    "    for i,r in enumerate(results,1):\n",
    "        t=r.get(\"title\") or \"Untitled\"\n",
    "        u=r.get(\"url\") or \"\"\n",
    "        c=_normalize(r.get(\"content\") or extra.get(u) or r.get(\"snippet\") or \"\")\n",
    "        if not c: continue\n",
    "        if len(c)>12000: c=c[:12000]+\" ...\"\n",
    "        arts.append({\"source_id\":i,\"title\":t,\"url\":u,\"content\":c})\n",
    "\n",
    "    if not arts: return {\"error\":\"No extractable content from references.\"}\n",
    "    return {\"raw_results\":res,\"articles\":arts}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "112c8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def persist_articles(arts, base=\"data\", stem=\"tavily_docs\"):\n",
    "    Path(base).mkdir(parents=True,exist_ok=True)\n",
    "    jp=Path(base)/f\"{stem}.json\"\n",
    "    xp=Path(base)/f\"{stem}.xlsx\"\n",
    "    with open(jp,\"w\",encoding=\"utf-8\") as f: json.dump(arts,f,indent=2,ensure_ascii=False)\n",
    "    pd.DataFrame(arts)[[\"source_id\",\"title\",\"url\",\"content\"]].to_excel(xp,index=False)\n",
    "    return str(jp),str(xp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "440926c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def _chunks(t,size=600,ov=120):\n",
    "    t=re.sub(r\"\\s+\",\" \",t).strip();out=[];i=0\n",
    "    while i<len(t):\n",
    "        out.append(t[i:i+size]);i+=max(1,size-ov)\n",
    "    return out\n",
    "\n",
    "def build_index(arts):\n",
    "    passages=[]\n",
    "    for a in arts:\n",
    "        for ch in _chunks(a[\"content\"]):\n",
    "            if len(ch)<200: continue\n",
    "            passages.append({\"text\":ch,\"source_id\":a[\"source_id\"],\n",
    "                             \"title\":a[\"title\"],\"url\":a[\"url\"]})\n",
    "    vec=TfidfVectorizer(ngram_range=(1,2),max_df=0.9,min_df=2,stop_words=\"english\")\n",
    "    mat=vec.fit_transform([p[\"text\"] for p in passages])\n",
    "    return vec,mat,passages\n",
    "\n",
    "def retrieve_passages(q,vec,mat,passages,k=8):\n",
    "    qv=vec.transform([q]);sims=cosine_similarity(qv,mat)[0]\n",
    "    idx=sims.argsort()[::-1][:k]\n",
    "    return [(passages[i],float(sims[i])) for i in idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "191f5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state:AgentState)->AgentState:\n",
    "    if state.get(\"error\"): return {\"answer\":f\"**Error:** {state['error']}\"}\n",
    "    arts=state.get(\"articles\") or []\n",
    "    if not arts:\n",
    "        res=state.get(\"raw_results\") or {}\n",
    "        rs=res.get(\"results\") or []\n",
    "        for i,r in enumerate(rs,1):\n",
    "            if r.get(\"snippet\"): arts.append(\n",
    "                {\"source_id\":i,\"title\":r.get(\"title\"),\"url\":r.get(\"url\"),\n",
    "                 \"content\":r.get(\"snippet\")})\n",
    "        if not arts: return {\"answer\":\"No sources to process.\"}\n",
    "\n",
    "    j,x=persist_articles(arts)\n",
    "    try: vec,mat,passages=build_index(arts)\n",
    "    except Exception as e: return {\"answer\":f\"Index error: {e}\"}\n",
    "\n",
    "    hits=retrieve_passages(state.get(\"question\",\"\"),vec,mat,passages,k=10)\n",
    "\n",
    "    pool=[]\n",
    "    for p,_ in hits:\n",
    "        for s in _SENT_SPLIT.split(_normalize(p[\"text\"])):\n",
    "            if 40<=len(s)<=400:\n",
    "                pool.append((p[\"source_id\"],s,p[\"title\"],p[\"url\"]))\n",
    "    if not pool: return {\"answer\":\"No usable sentences in retrieved chunks.\"}\n",
    "\n",
    "    sents=[x[1] for x in pool]\n",
    "    sc=_score_sents(sents); pick=_mmr(sents,sc,6,0.7)\n",
    "    used={};lines=[]\n",
    "    for i in pick:\n",
    "        sid,s,ttl,url=pool[i];used[sid]=(ttl,url)\n",
    "        lines.append(f\"{s} [{sid}]\")\n",
    "    summ=\" \".join(lines)\n",
    "\n",
    "    srcs={a[\"source_id\"]:(a[\"title\"],a[\"url\"]) for a in arts}\n",
    "    src_txt=\"\\n\".join(f\"{sid}. [{t}]({u})\" for sid,(t,u) in srcs.items())\n",
    "    ans=f\"{summ}\\n\\n**Sources**\\n{src_txt}\\n\\n_JSON_: `{j}` , _Excel_: `{x}`\"\n",
    "    return {\"answer\":ans}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0aa8e733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Shaheen is reportedly part of Al-Falah University and closely associated with Kashmiri doctor Muzammil Ganaie, alias Musaib, who was arrested after 2,900 kg of explosives and inflammable material were recovered from his two rented rooms in Faridabad. [3] [...] A Lucknow-based woman doctor, arrested in connection with a massive explosives haul in Faridabad near Delhi, was tasked with establishing the women's wing of the Pakistan-based terror group Jaish-e-Mohammed (JeM) in India, according to Delhi Police sources. [3] A Lucknow-based woman doctor, arrested in connection with a massive explosives haul in Faridabad near Delhi, was tasked with establishing the women's wing of the Pakistan-based terror group Jaish-e-Mohammed (JeM) in India, according to Delhi Police sources. [5] ### Description 21913 views Posted: 10 Nov 2025 A joint operation by the Jammu and Kashmir Police, the Intelligence Bureau, and the Faridabad Police has foiled what could have been a major terror attack near Delhi. [6] [...] Lucknow doctor Shaheena Shahid led Jaish-e-Mohammed's women's wing in India, according to sources She was arrested after the Faridabad explosives haul, and an assault rifle was found in her car Shaheena is linked to Al-Falah University and is an associate of the arrested Kashmiri doctor Muzammil Ganaie Did our AI summary help? [3] Shakeel and another Kashmiri doctor, Adeel Ahmad Rather, were among eight people who were arrested for allegedly being part of a \"white-collar terror\" module involving the Jaish-e-Mohammed and Ansar Ghazwat-ul-Hind and spanning Jammu and Kashmir, Haryana, and Uttar Pradesh. [1]\n",
       "\n",
       "**Sources**\n",
       "1. [2 Arrested, 50 Kg More Explosives Recovered In Faridabad ... - NDTV](https://www.ndtv.com/video/2-arrested-50-kg-more-explosives-recovered-in-faridabad-day-after-delhi-blast-1020620)\n",
       "2. [Terror plot: Med prof's arrest leads to 2900kg explosives haul in ...](https://timesofindia.indiatimes.com/india/terror-plot-med-profs-arrest-leads-to-2900kg-explosives-haul-in-fbd/articleshow/125237164.cms)\n",
       "3. [Arrested UP Doctor Was Tasked With Setting Up Jaish's ... - NDTV](https://www.ndtv.com/india-news/dr-shaheena-shahid-jamaat-ul-mominat-india-head-lucknow-doctor-arrested-in-faridabad-terror-module-was-head-of-jaish-e-mohammed-jem-women-wing-in-indi-9613556)\n",
       "4. [J&K terror module busted: Two Kashmiri doctors linked to Jaish,](https://indianexpress.com/article/india/explosives-in-faridabad-doctors-house-transnational-terror-module-busted-j-k-police-probe-10356894/)\n",
       "5. [Arrested UP Doctor Was Tasked With Setting Up Jaish's Women ...](https://www.ndtv.com/video/arrested-up-doctor-was-tasked-with-setting-up-jaish-s-women-wing-in-india-1020649)\n",
       "6. [Faridabad terror plot foiled 350 kg of explosives and arms recovered ...](https://www.youtube.com/watch?v=owNkemS_sEY)\n",
       "7. [J&K Doctor With Ammonium Nitrate Arrested Near Delhi](https://www.ndtv.com/india-news/350kg-ammonium-nitrate-explosive-found-with-j-k-doctor-in-faridabad-near-delhi-what-is-ammonium-nitrate-is-ammonium-nitrate-dangerous-9606804)\n",
       "8. [Doctors of Doom Under Scanner: Investigating Links to Delhi Blast](https://www.instagram.com/p/DQ6YqoYExmy/)\n",
       "\n",
       "_JSON_: `data\\tavily_docs.json` , _Excel_: `data\\tavily_docs.xlsx`"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_agent():\n",
    "    g=StateGraph(AgentState)\n",
    "    g.add_node(\"search\",search_web)\n",
    "    g.add_node(\"result\",generate_answer)\n",
    "    g.set_entry_point(\"search\")\n",
    "    g.add_edge(\"search\",\"result\")\n",
    "    g.add_edge(\"result\",END)\n",
    "    return g.compile()\n",
    "\n",
    "agent=create_agent()\n",
    "\n",
    "# example\n",
    "out=agent.invoke({\"question\":\"tell me about the doctor arrested with explosive in faridabad\"})\n",
    "display_markdown(out[\"answer\"],raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c07d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizerenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
