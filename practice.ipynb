{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b9c722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langgraph.graph import Graph, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from datetime import datetime\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from newsapi import NewsApiClient\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, Image as IPImage\n",
    "from langchain_core.messages import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43206390",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Use GPT-4o-mini\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    news_query: Annotated[str, \"Input query to extract news search parameters from.\"]\n",
    "    num_searches_remaining: Annotated[int, \"Number of searches remaining.\"]\n",
    "    newsapi_params: Annotated[dict, \"Structured argument for the News API.\"]\n",
    "    past_searches: Annotated[List[dict], \"List of search params already used.\"]\n",
    "    articles_metadata: Annotated[List[dict], \"Article metadata response from the News API.\"]\n",
    "    scraped_urls: Annotated[List[str], \"List of urls already scraped.\"]\n",
    "    num_articles_tldr: Annotated[int, \"Number of articles to create TL;DR for.\"]\n",
    "    potential_articles: Annotated[List[dict], \"Articles with full text to consider summarizing.\"]\n",
    "    tldr_articles: Annotated[List[dict], \"Selected article TL;DRs.\"]\n",
    "    formatted_results: Annotated[str, \"Formatted results to display.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c288cf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsApiParams(BaseModel):\n",
    "    q: str = Field(description=\"1-3 concise keyword search terms that are not too specific\")\n",
    "    sources: str =Field(description=\"comma-separated list of sources from: 'abc-news,abc-news-au,associated-press,australian-financial-review,axios,bbc-news,bbc-sport,bloomberg,business-insider,cbc-news,cbs-news,cnn,financial-post,fortune'\")\n",
    "    from_param: str = Field(description=\"date in format 'YYYY-MM-DD' Two days ago minimum. Extend up to 30 days on second and subsequent requests.\")\n",
    "    to: str = Field(description=\"date in format 'YYYY-MM-DD' today's date unless specified\")\n",
    "    language: str = Field(description=\"language of articles 'en' unless specified one of ['ar', 'de', 'en', 'es', 'fr', 'he', 'it', 'nl', 'no', 'pt', 'ru', 'se', 'ud', 'zh']\")\n",
    "    sort_by: str = Field(description=\"sort by 'relevancy', 'popularity', or 'publishedAt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b50ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. generate_newsapi_params\n",
    "def generate_newsapi_params(state: dict):\n",
    "    \"\"\"Generate News API params from the user's query, then enforce a tight, fresh search.\"\"\"\n",
    "    today_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    news_query = state['news_query']\n",
    "    num_searches_remaining = state['num_searches_remaining']\n",
    "    past_searches = state['past_searches']\n",
    "    last_req = state.get(\"last_newsapi_request\")\n",
    "    sys_prompt = \"\"\"\n",
    "    Today's date is {today_date}.\n",
    "    Create a param dict for the News API on the user query:\n",
    "    {query}\n",
    "\n",
    "    These searches have already been made. Loosen the search terms to get more results:\n",
    "    {past_searches}\n",
    "\n",
    "    Including this one, you have {num_searches_remaining} searches remaining.\n",
    "    If this is your last search, use all news resources and a 30-day search range.\n",
    "    \"\"\"\n",
    "    sys_msg = sys_prompt.format(\n",
    "        today_date=today_date,\n",
    "        query=news_query,\n",
    "        past_searches=past_searches,\n",
    "        num_searches_remaining=num_searches_remaining\n",
    "    )\n",
    "\n",
    "    llm_with_news_structured_output = llm.with_structured_output(NewsApiParams)\n",
    "    result = llm_with_news_structured_output.invoke([SystemMessage(content=sys_msg)])\n",
    "\n",
    "    # Base params from LLM\n",
    "    params = {\n",
    "        'q': result.q,\n",
    "        'sources': result.sources,\n",
    "        'from_param': result.from_param,\n",
    "        'to': result.to,\n",
    "        'language': result.language,\n",
    "        'sort_by': result.sort_by\n",
    "    }\n",
    "\n",
    "    q_lower = news_query.lower()\n",
    "\n",
    "    # --- Smart quoting for model-like queries (contains both letters & numbers)\n",
    "    import re\n",
    "    if re.search(r\"[a-zA-Z]+\\s*\\d\", q_lower):\n",
    "        params[\"q\"] = f\"\\\"{news_query}\\\" OR {news_query.split()[0]}\"\n",
    "        params.pop(\"sources\", None)\n",
    "\n",
    "    # --- Broaden if we’re repeating the same params as last time\n",
    "    if last_req and params.get(\"q\") == last_req.get(\"q\") and params.get(\"sources\") == last_req.get(\"sources\"):\n",
    "        params.pop(\"sources\", None)\n",
    "        if \"OR\" not in params[\"q\"]:\n",
    "            params[\"q\"] = f\"{params['q']} OR {news_query}\"\n",
    "\n",
    "    state[\"newsapi_params\"] = params\n",
    "    return state\n",
    "\n",
    "    # ✅ Prefer most-recent\n",
    "    params['sort_by'] = 'publishedAt'\n",
    "    state['newsapi_params'] = params\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e4b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, UTC\n",
    "\n",
    "VALID_SORT = {\"relevancy\", \"popularity\", \"publishedAt\"}\n",
    "\n",
    "def _parse_date_yyyy_mm_dd(d):\n",
    "    if not d:\n",
    "        return None\n",
    "    s = str(d)[:10]\n",
    "    try:\n",
    "        return datetime.fromisoformat(s).date()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _clamp_30_days(from_param, to_param):\n",
    "    today = datetime.now(UTC).date()\n",
    "    max_age = today - timedelta(days=30)\n",
    "    t = _parse_date_yyyy_mm_dd(to_param) or today\n",
    "    if t > today:\n",
    "        t = today\n",
    "    f = _parse_date_yyyy_mm_dd(from_param) or max_age\n",
    "    if f < max_age:\n",
    "        f = max_age\n",
    "    if f > t:\n",
    "        f = t - timedelta(days=1)\n",
    "    return f.isoformat(), t.isoformat()\n",
    "\n",
    "def _clean_and_validate_params(newsapi, params: dict) -> dict:\n",
    "    q = (params.get(\"q\") or \"\").strip()\n",
    "    language = (params.get(\"language\") or \"en\").strip()\n",
    "    sort_by = (params.get(\"sort_by\") or \"publishedAt\").strip()\n",
    "    if sort_by not in VALID_SORT:\n",
    "        sort_by = \"publishedAt\"\n",
    "\n",
    "    from_param, to_param = _clamp_30_days(params.get(\"from_param\"), params.get(\"to\"))\n",
    "\n",
    "    requested_sources = params.get(\"sources\")\n",
    "    cleaned_sources = None\n",
    "    if requested_sources:\n",
    "        if isinstance(requested_sources, (list, tuple, set)):\n",
    "            requested_sources = \",\".join([s for s in requested_sources if s])\n",
    "        elif isinstance(requested_sources, str):\n",
    "            requested_sources = \",\".join([s.strip() for s in requested_sources.split(\",\") if s.strip()])\n",
    "        else:\n",
    "            requested_sources = None\n",
    "\n",
    "        if requested_sources:\n",
    "            try:\n",
    "                src_resp = newsapi.get_sources(language=language)\n",
    "                valid_ids = {s[\"id\"] for s in src_resp.get(\"sources\", []) if s.get(\"id\")}\n",
    "                keep = [s for s in requested_sources.split(\",\") if s in valid_ids]\n",
    "                if keep:\n",
    "                    cleaned_sources = \",\".join(keep)\n",
    "            except Exception:\n",
    "                cleaned_sources = None\n",
    "\n",
    "    # Python SDK param names\n",
    "    safe = {\n",
    "        \"language\": language,\n",
    "        \"sort_by\": sort_by,\n",
    "        \"from_param\": from_param,\n",
    "        \"to\": to_param,\n",
    "        \"page_size\": 50,\n",
    "    }\n",
    "    if q:\n",
    "        safe[\"q\"] = q\n",
    "    # keep sources only if validated; otherwise omit for broader coverage\n",
    "    if cleaned_sources:\n",
    "        safe[\"sources\"] = cleaned_sources\n",
    "\n",
    "    if \"q\" not in safe and \"sources\" not in safe:\n",
    "        safe[\"q\"] = \"news\"\n",
    "    return safe\n",
    "\n",
    "def _dec(state: dict) -> None:\n",
    "    state[\"num_searches_remaining\"] = max(0, int(state.get(\"num_searches_remaining\", 0)) - 1)\n",
    "\n",
    "def retrieve_article_metadata(state: dict) -> dict:\n",
    "    state.setdefault(\"articles_metadata\", [])\n",
    "    state.setdefault(\"past_searches\", [])\n",
    "    state.setdefault(\"potential_articles\", [])\n",
    "    state.setdefault(\"scraped_urls\", set())\n",
    "\n",
    "    newsapi_params = dict(state.get(\"newsapi_params\", {}))\n",
    "    scraped_urls = set(state.get(\"scraped_urls\") or [])\n",
    "    potential_articles = state.get(\"potential_articles\") or []\n",
    "    past_searches = state.get(\"past_searches\") or []\n",
    "\n",
    "    api_key = os.getenv(\"NEWS_API_KEY\")\n",
    "    if not api_key:\n",
    "        state[\"last_newsapi_error\"] = \"NEWS_API_KEY not set\"\n",
    "        state[\"articles_metadata\"] = []\n",
    "        _dec(state)\n",
    "        return state\n",
    "    newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "    safe_params = _clean_and_validate_params(newsapi, newsapi_params)\n",
    "    state[\"last_newsapi_request\"] = dict(safe_params)\n",
    "\n",
    "    try:\n",
    "        res = newsapi.get_everything(**safe_params)\n",
    "    except Exception as e:\n",
    "        state[\"last_newsapi_error\"] = f\"Request exception: {e}\"\n",
    "        state[\"articles_metadata\"] = []\n",
    "        _dec(state)\n",
    "        return state\n",
    "\n",
    "    if not isinstance(res, dict) or res.get(\"status\") != \"ok\":\n",
    "        msg = res.get(\"message\", \"Unknown NewsAPI error\") if isinstance(res, dict) else \"Invalid response\"\n",
    "        state[\"last_newsapi_error\"] = f\"API error: {msg}\"\n",
    "        state[\"articles_metadata\"] = []\n",
    "        _dec(state)\n",
    "        return state\n",
    "\n",
    "    if not past_searches or past_searches[-1] != dict(safe_params):\n",
    "        past_searches.append(dict(safe_params))\n",
    "\n",
    "    target_total = 10\n",
    "    remaining_slots = max(0, target_total - len(potential_articles))\n",
    "    new_articles = []\n",
    "\n",
    "    for article in res.get(\"articles\", []):\n",
    "        url = (article or {}).get(\"url\")\n",
    "        if not url or url in scraped_urls:\n",
    "            continue\n",
    "        if len(new_articles) >= remaining_slots:\n",
    "            break\n",
    "        new_articles.append(article)\n",
    "\n",
    "    state[\"articles_metadata\"] = new_articles\n",
    "    state[\"past_searches\"] = past_searches\n",
    "    state[\"last_newsapi_error\"] = None\n",
    "    _dec(state)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d2ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Soften filters\n",
    "PAYWALLED_OR_FUSSY = {\n",
    "    \"bloomberg.com\", \"wsj.com\", \"ft.com\", \"nytimes.com\",\n",
    "    \"economist.com\", \"washingtonpost.com\"\n",
    "}\n",
    "ANTI_BOT_MARKERS = [\n",
    "    \"are you a robot\", \"enable javascript\", \"turn on javascript\",\n",
    "    \"cookie policy\", \"terms of service\", \"subscribe to\", \"paywall\"\n",
    "]\n",
    "MIN_TEXT_LEN = 200   # was 800\n",
    "\n",
    "def _looks_bad(text: str) -> bool:\n",
    "    t = text.lower()\n",
    "    return any(marker in t for marker in ANTI_BOT_MARKERS)\n",
    "\n",
    "def _domain(url: str) -> str:\n",
    "    try:\n",
    "        return url.split(\"//\", 1)[1].split(\"/\", 1)[0].replace(\"www.\", \"\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def retrieve_articles_text(state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"Fetch full text for each article and populate potential_articles with good content.\"\"\"\n",
    "    state = dict(state or {})\n",
    "    articles_meta = state.get(\"articles_metadata\") or []\n",
    "    scraped_urls = set(state.get(\"scraped_urls\") or [])\n",
    "    potential = state.get(\"potential_articles\") or []\n",
    "    max_to_fetch = max(0, 10 - len(potential))\n",
    "\n",
    "    state[\"last_scrape_error\"] = None\n",
    "\n",
    "    if not isinstance(articles_meta, list):\n",
    "        state[\"articles_metadata\"] = []\n",
    "        state[\"last_scrape_error\"] = \"articles_metadata not list\"\n",
    "        return state\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/121.0.0.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    }\n",
    "\n",
    "    new_items = []\n",
    "    for art in articles_meta:\n",
    "        if len(new_items) >= max_to_fetch:\n",
    "            break\n",
    "        url = (art or {}).get(\"url\")\n",
    "        if not url or url in scraped_urls:\n",
    "            continue\n",
    "\n",
    "        dom = _domain(url)\n",
    "\n",
    "        # Try network scrape unless the domain is notoriously hostile\n",
    "        scraped_ok = False\n",
    "        text = \"\"\n",
    "        if dom not in PAYWALLED_OR_FUSSY:\n",
    "            try:\n",
    "                resp = requests.get(url, timeout=10, headers=headers)\n",
    "                if resp.status_code == 200 and resp.text:\n",
    "                    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "                    text = \" \".join(t.strip() for t in soup.stripped_strings)\n",
    "                    if text and not _looks_bad(text) and len(text) >= MIN_TEXT_LEN:\n",
    "                        scraped_ok = True\n",
    "            except Exception as e:\n",
    "                state[\"last_scrape_error\"] = f\"scrape error for {url}: {e}\"\n",
    "\n",
    "        # Fallback to NewsAPI fields if scrape failed or is paywalled\n",
    "        if not scraped_ok:\n",
    "            fallback_text = ((art.get(\"content\") or \"\") + \"\\n\" + (art.get(\"description\") or \"\")).strip()\n",
    "            if len(fallback_text) >= 150:  # was 400\n",
    "                text = fallback_text\n",
    "                scraped_ok = True\n",
    "\n",
    "        if not scraped_ok:\n",
    "            scraped_urls.add(url)\n",
    "            continue\n",
    "\n",
    "        new_items.append({\n",
    "            \"title\": art.get(\"title\") or \"\",\n",
    "            \"url\": url,\n",
    "            \"text\": text[:20000],\n",
    "            \"publishedAt\": art.get(\"publishedAt\", \"\"),\n",
    "            \"description\": art.get(\"description\", \"\")\n",
    "        })\n",
    "        scraped_urls.add(url)\n",
    "\n",
    "    state[\"potential_articles\"] = potential + new_items\n",
    "    state[\"scraped_urls\"] = list(scraped_urls)\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab0e2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_text_decision(state: Dict[str, Any]) -> str:\n",
    "    # If we have at least one scraped article, proceed to select top urls\n",
    "    if state.get(\"potential_articles\"):\n",
    "        return \"select_top_urls\"\n",
    "\n",
    "    # If we never got metadata, either broaden or end\n",
    "    if not state.get(\"articles_metadata\"):\n",
    "        # try again if you still have searches left\n",
    "        if state.get(\"num_searches_remaining\", 0) > 1:\n",
    "            return \"generate_newsapi_params\"\n",
    "        return \"END\"\n",
    "\n",
    "    # We had metadata but got no text (paywall, scraper failed, etc.)\n",
    "    return \"select_top_urls\" if state.get(\"articles_metadata\") else \"END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd20048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_urls(state: GraphState) -> GraphState:\n",
    "    \"\"\"Based on the article synopses, choose the top-n articles to summarize.\"\"\"\n",
    "    news_query = state[\"news_query\"]\n",
    "    num_articles_tldr = state[\"num_articles_tldr\"]\n",
    "\n",
    "    # processed articles \n",
    "    potential_articles = state.get(\"potential_articles\", [])\n",
    "\n",
    "    # format the metadata\n",
    "    formatted_metadata = \"\\n\".join(\n",
    "        f\"{article.get('url','')}\\n{article.get('description','')}\\n\"\n",
    "        for article in potential_articles\n",
    "        if article.get(\"url\")\n",
    "    )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Based on the user news query:\n",
    "    {news_query}\n",
    "\n",
    "    Reply with a list of strings of up to {num_articles_tldr} relevant urls.\n",
    "    Don't add any urls that are not relevant or aren't listed specifically.\n",
    "    {formatted_metadata}\n",
    "    \"\"\"\n",
    "    result = llm.invoke(prompt).content\n",
    "\n",
    "    # extract URLs\n",
    "    url_pattern = r'(https?://[^\\s\",]+)'\n",
    "    urls = re.findall(url_pattern, result)\n",
    "\n",
    "    # keep only selected articles\n",
    "    tldr_articles = [a for a in potential_articles if a.get(\"url\") in urls]\n",
    "\n",
    "    state[\"tldr_articles\"] = tldr_articles\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134fae5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def summarize_articles_parallel(state: GraphState) -> GraphState:\n",
    "    \"\"\"Summarize the articles concurrently based on full text.\"\"\"\n",
    "    tldr_articles = state.get(\"tldr_articles\", [])\n",
    "\n",
    "    prompt_template = \"\"\"\n",
    "    Create a * bulleted summarizing tldr for the article:\n",
    "    {text}\n",
    "\n",
    "    Be sure to follow the following format exactly with nothing else:\n",
    "    {title}\n",
    "    {url}\n",
    "    * tl;dr bulleted summary\n",
    "    * use bullet points for each sentence\n",
    "    \"\"\"\n",
    "\n",
    "    async def summarize_article(article):\n",
    "        text = article.get(\"text\", \"\")\n",
    "        title = article.get(\"title\", \"\")\n",
    "        url = article.get(\"url\", \"\")\n",
    "        prompt = prompt_template.format(title=title, url=url, text=text)\n",
    "        result = await llm.ainvoke(prompt)\n",
    "        article[\"summary\"] = result.content\n",
    "        return article\n",
    "\n",
    "    if tldr_articles:\n",
    "        tldr_articles = await asyncio.gather(*[summarize_article(a) for a in tldr_articles])\n",
    "\n",
    "    state[\"tldr_articles\"] = tldr_articles\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8999093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(state: GraphState) -> GraphState:\n",
    "    \"\"\"Format the results for display.\"\"\"\n",
    "    # load a list of past search queries\n",
    "    q = [newsapi_params[\"q\"] for newsapi_params in state[\"past_searches\"]]\n",
    "    formatted_results = f\"Here are the top {len(state['tldr_articles'])} articles based on search terms:\\\\n{', '.join(q)}\\\\n\\\\n\"\n",
    "\n",
    "    # load the summarized articles\n",
    "    tldr_articles = state[\"tldr_articles\"]\n",
    "\n",
    "    # format article tl;dr summaries\n",
    "    tldr_articles = \"\\\\n\\\\n\".join([f\"{article['summary']}\" for article in tldr_articles])\n",
    "\n",
    "    # concatenate summaries to the formatted results\n",
    "    formatted_results += tldr_articles\n",
    "\n",
    "    state[\"formatted_results\"] = formatted_results\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9bec6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(state: GraphState) -> GraphState:\n",
    "    \"\"\"Format the results for display.\"\"\"\n",
    "    # load a list of past search queries\n",
    "    q = [params.get(\"q\", \"\") for params in state.get(\"past_searches\", [])]\n",
    "    formatted_results = (\n",
    "        f\"Here are the top {len(state['tldr_articles'])} articles based on search terms:\\n\"\n",
    "        f\"{', '.join(q)}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    # load the summarized articles\n",
    "    tldr_articles = state[\"tldr_articles\"]\n",
    "\n",
    "    # format article tl;dr summaries\n",
    "    tldr_articles = \"\\n\\n\".join([f\"{article['summary']}\" for article in tldr_articles])\n",
    "\n",
    "    # concatenate summaries to the formatted results\n",
    "    formatted_results += tldr_articles\n",
    "\n",
    "    state[\"formatted_results\"] = formatted_results\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b95e7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_text_decision(state: GraphState) -> str:\n",
    "    \"\"\"Route after retrieve_articles_text with strict stop conditions.\"\"\"\n",
    "    remaining = int(state.get(\"num_searches_remaining\", 0))\n",
    "    have = len(state.get(\"potential_articles\", []) or [])\n",
    "    need = int(state.get(\"num_articles_tldr\", 0))\n",
    "\n",
    "    # If we already have enough content, proceed to selection\n",
    "    if have >= max(1, need):\n",
    "        return \"select_top_urls\"\n",
    "\n",
    "    # No searches left: end if empty, otherwise proceed with what we have\n",
    "    if remaining <= 0:\n",
    "        if have == 0:\n",
    "            state[\"formatted_results\"] = \"No articles with text found.\"\n",
    "            return \"END\"\n",
    "        return \"select_top_urls\"\n",
    "\n",
    "    # Still have searches left but not enough content -> broaden & retry\n",
    "    return \"generate_newsapi_params\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe228d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END\n",
    "\n",
    "def make_app() -> Graph:\n",
    "    workflow = Graph()\n",
    "\n",
    "    # nodes\n",
    "    workflow.add_node(\"generate_newsapi_params\", generate_newsapi_params)\n",
    "    workflow.add_node(\"retrieve_articles_metadata\", retrieve_article_metadata)\n",
    "    workflow.add_node(\"retrieve_articles_text\", retrieve_articles_text)\n",
    "    workflow.add_node(\"select_top_urls\", select_top_urls)\n",
    "    workflow.add_node(\"summarize_articles_parallel\", summarize_articles_parallel)\n",
    "    workflow.add_node(\"format_results\", format_results)\n",
    "\n",
    "    # entrypoint\n",
    "    workflow.add_edge(START, \"generate_newsapi_params\")\n",
    "\n",
    "    # edges\n",
    "    workflow.add_edge(\"generate_newsapi_params\", \"retrieve_articles_metadata\")\n",
    "    workflow.add_edge(\"retrieve_articles_metadata\", \"retrieve_articles_text\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"retrieve_articles_text\",\n",
    "        articles_text_decision,\n",
    "        {\n",
    "            \"generate_newsapi_params\": \"generate_newsapi_params\",\n",
    "            \"select_top_urls\": \"select_top_urls\",\n",
    "            \"END\": END,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"select_top_urls\", \"summarize_articles_parallel\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"summarize_articles_parallel\",\n",
    "        lambda state: \"format_results\" if len(state.get(\"tldr_articles\", [])) > 0 else \"END\",\n",
    "        {\n",
    "            \"format_results\": \"format_results\",\n",
    "            \"END\": END,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"format_results\", END)\n",
    "    return workflow.compile()\n",
    "\n",
    "# Build the app (this must run before calling run_workflow)\n",
    "app = make_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a08b6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_workflow(query: str, num_searches_remaining: int = 3, num_articles_tldr: int = 2):\n",
    "    \"\"\"Run the LangGraph workflow and display results.\"\"\"\n",
    "    initial_state = {\n",
    "        \"news_query\": query,\n",
    "        \"num_searches_remaining\": num_searches_remaining,\n",
    "        \"newsapi_params\": {},\n",
    "        \"past_searches\": [],\n",
    "        \"articles_metadata\": [],\n",
    "        \"scraped_urls\": [],\n",
    "        \"num_articles_tldr\": num_articles_tldr,\n",
    "        \"potential_articles\": [],\n",
    "        \"tldr_articles\": [],\n",
    "        \"formatted_results\": \"No articles with text found.\"\n",
    "    }\n",
    "    try:\n",
    "        # Add recursion_limit to prevent infinite loops\n",
    "        result = await app.ainvoke(initial_state, config={\"recursion_limit\": 50})\n",
    "        return result[\"formatted_results\"]\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3518e32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last_newsapi_request = {'language': 'en', 'sort_by': 'publishedAt', 'from_param': '2025-10-11', 'to': '2025-11-10', 'page_size': 50, 'q': 'Delhi Red Fort incident', 'sources': 'abc-news,bbc-news,cnn,associated-press'}\n",
      "last_newsapi_error   = None\n",
      "last_scrape_error    = None\n",
      "metadata len         = 0\n",
      "potential len        = 0\n",
      "tldr len             = 0\n"
     ]
    }
   ],
   "source": [
    "diag_state = await app.ainvoke({\n",
    "    \"news_query\": \"delhi red fort blast\",\n",
    "    \"num_searches_remaining\": 2,\n",
    "    \"newsapi_params\": {},\n",
    "    \"past_searches\": [],\n",
    "    \"articles_metadata\": [],\n",
    "    \"scraped_urls\": [],\n",
    "    \"num_articles_tldr\": 3,\n",
    "    \"potential_articles\": [],\n",
    "    \"tldr_articles\": [],\n",
    "    \"formatted_results\": \"\"\n",
    "}, config={\"recursion_limit\": 40})\n",
    "\n",
    "print(\"last_newsapi_request =\", diag_state.get(\"last_newsapi_request\"))\n",
    "print(\"last_newsapi_error   =\", diag_state.get(\"last_newsapi_error\"))\n",
    "print(\"last_scrape_error    =\", diag_state.get(\"last_scrape_error\"))\n",
    "print(\"metadata len         =\", len(diag_state.get(\"articles_metadata\", [])))\n",
    "print(\"potential len        =\", len(diag_state.get(\"potential_articles\", [])))\n",
    "print(\"tldr len             =\", len(diag_state.get(\"tldr_articles\", [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07d9098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles with text found.\n"
     ]
    }
   ],
   "source": [
    "query = \"delhi red fort blast\"\n",
    "result=await run_workflow(query, num_articles_tldr=3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f5579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizerenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
